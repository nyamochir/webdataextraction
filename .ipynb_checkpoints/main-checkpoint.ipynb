{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the passion project that every body can use this source code advance their task -->\n",
    "this can help you to get data from the website using the python selenium -- and extract the specific data \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import selenium\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "import time \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime# importing entire datetime library --\n",
    "\n",
    "import os\n",
    "# result data will be thrown into the data folder\n",
    "try:\n",
    "    os.mkdir(\"data\")\n",
    "except:\n",
    "    pass\n",
    "def get_data():\n",
    "    options = webdriver.ChromeOptions() # there are some runs that can --> \n",
    "    # -- options.ad(\"start-maximized\") \n",
    "    # run the rest of the models --> that can be maximed -- .\n",
    "    options.add_argument(\"start-maximized\") # data valivation process will \n",
    "\n",
    "    # webdriver has be in side this column -- that can run all the rest of the  cases --> \n",
    "    browser = webdriver.Chrome(options = options)\n",
    "\n",
    "    browser.get(\"https://www.worldometers.info/coronavirus/\")\n",
    "    time.sleep(6)# lets' just load for 5 mins -- that can wait the data to \n",
    "\n",
    "    # get -- particular days content plus, get the contents from the last days\n",
    "    content1 = browser.page_source# need to get the page source that can be obtained -- seems -->\n",
    "\n",
    "    browser.find_element_by_xpath('//*[@id=\"nav-yesterday-tab\"]/a').click()# data click to parse more data --> \n",
    "    time.sleep(4)\n",
    "\n",
    "    content2 = browser.page_source# need to get the page source that can be obtained -->  run those last models --> \n",
    "    browser.close()\n",
    "    return content1, content2# main page that is creating today's data and page that is containing yesterday's data -->\n",
    "\n",
    "\n",
    "def getcoronatable_using_requests():\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    get current table data extraction using \n",
    "    This function will get the data from https://www.worldometers.info/coronavirus/ sites -- and synchronize it in the terms.\n",
    "    \"\"\"\n",
    "    response = requests.get(\"https://www.worldometers.info/coronavirus/\")\n",
    "    print(response.status_code)\n",
    "    namepart = str(datetime.datetime.now())\n",
    "    if response.status_code == 200:\n",
    "        print(\"Successful load of the website.\") # there is the successful load from the website -- will get printed --> \n",
    "    \n",
    "    # using BeautifulSoup the parse the content in response\n",
    "    bs_content = BeautifulSoup(response.content, \"html\")\n",
    "    \n",
    "    data_in_html = bs_content.find_all(\"table\") # finding all the tables \n",
    "\n",
    "    rows = data_in_html[0].find_all(\"tr\") # all the features those are columns -\n",
    "    columns = []\n",
    "    for feature in rows[0].text.split(\"\\n\"):\n",
    "        if feature!= \"\":\n",
    "            columns.append(feature)\n",
    "    #data = pd.DataFrame(columns = columns)\n",
    "    #data.head()\n",
    "    print(columns, len(columns))\n",
    "    data_rows=  []\n",
    "    rows = rows[1:]  \n",
    "    \n",
    "    for row in rows:\n",
    "        row = row.text.split(\"\\n\")[1:]\n",
    "        row = row[:-1]\n",
    "        date_data = row[-1]\n",
    "        \n",
    "        row = row[:-2]\n",
    "        row.append(date_data)# append is proper use\n",
    "        # making sure that there is no column  that are in different shape \n",
    "        if len(row) == len(columns):\n",
    "            \n",
    "            data_rows.append(row)\n",
    "    print(len(data_rows))\n",
    "    # this  live -->  coding from --\n",
    "    print(\"The length of the each row that is extracted: \", len(data_rows[0]))# can see the characteristics of the table presentented here ..\n",
    "    \n",
    "    print(\"Data inside the first row is : \", data_rows[0])\n",
    "    print(\"Data inside the second row is : \", data_rows[1])\n",
    "    print(\"Data inside the third row is : \", data_rows[2])\n",
    "    data = pd.DataFrame(data_rows, columns = columns)\n",
    "    data.to_csv(\"data/data_used_requests.csv\", index = False, encoding = \"utf-8\")# will just not create the variable that is running on the current server  that can be loaded into the following values -->\n",
    "    print(\"data creation is successful\")\n",
    "    # encode the data by  utf-8 code * not mandatory --> here>\n",
    "    # if need can throw data --> as a variable\n",
    "    return None\n",
    "\n",
    "def extract_table_from_source(content, counter):\n",
    "    \"\"\"\n",
    "    gets an html content that is downloaded from the selenium extractor\n",
    "    recommeded consequence of the tables are  --> \n",
    "    first main table - that is containing today's data\n",
    "    second table that is containing data from yesterday\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    temp = BeautifulSoup(content, \"html\")\n",
    "    data_in_html = temp.find_all(\"table\" )\n",
    "    rows = data_in_html[0].find_all(\"tr\")\n",
    "    head = rows[0] # html cell is head = \n",
    "    body = rows[1:]\n",
    "    columns = []\n",
    "    for i in head.find_all(\"th\"):\n",
    "        print(i.text)#\n",
    "        columns.append(i.text) # the cells will be filled with values -->  that can be changed --- \n",
    "    main_table = []\n",
    "    # iteration through each cells\n",
    "    name = \"today\"\n",
    "    # naming for the datasets that is getting downloaded\n",
    "    if counter == 0:\n",
    "        name = name\n",
    "    else:\n",
    "        name = \"yesterday\"\n",
    "    \n",
    "    for row in body:\n",
    "        each_row =  []\n",
    "        for i in row.find_all(\"td\"):\n",
    "            each_row.append(i.text)\n",
    "        main_table.append(each_row)\n",
    "    frame = pd.DataFrame(main_table, columns = columns)\n",
    "    frame.to_csv(\"data_\"+name+\".csv\", index = False, encoding = \"utf-8\")#  run all the models that can't be run --\n",
    "    return frame\n",
    "def main():\n",
    "    content1, content2 = get_data()\n",
    "    # this is not required to download the pages there from the source   \n",
    "    # can use for the different purpose just using requests library -->\n",
    "    #getcoronatable_using_requests()\n",
    "    index = 0\n",
    "    for content in [content1, content2]:\n",
    "        extract_table_from_source(content, index)\n",
    "        index +=1\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
